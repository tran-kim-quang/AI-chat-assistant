import openai
import os
import base64
import pandas as pd
import requests
import io
from typing import Union, Dict, Any
from dotenv import load_dotenv

load_dotenv()
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
model_text = os.getenv('TEXT_MODEL')
model_img = os.getenv('PICTURE_MODEL')

# Process text
def text_process(content: str) -> str:
    """X·ª≠ l√Ω vƒÉn b·∫£n v·ªõi LLM"""
    try:
        client = openai.OpenAI(
            base_url="https://api.groq.com/openai/v1",
            api_key=GROQ_API_KEY
        )

        response = client.chat.completions.create(
            model=model_text,
            messages=[
                {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω ·∫£o th√¢n thi·ªán"},
                {"role": "user", "content": content}
            ]
        )

        return response.choices[0].message.content
    except Exception as e:
        return f"L·ªói x·ª≠ l√Ω vƒÉn b·∫£n: {str(e)}"

# Process img
def encode_image_to_base64(image_path: str) -> str:
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file ·∫£nh: {image_path}")
    
    with open(image_path, "rb") as image_file:
        encoded = base64.b64encode(image_file.read()).decode('utf-8')
    
    # X√°c ƒë·ªãnh lo·∫°i ·∫£nh
    ext = os.path.splitext(image_path)[1].lower()
    mime_type = "image/jpeg" if ext in ['.jpg', '.jpeg'] else "image/png"
    
    return f"data:{mime_type};base64,{encoded}"

def img_process(path: str, user_prompt: str = None) -> str:
    try:
        # Encode ·∫£nh
        base64_image = encode_image_to_base64(path)
        
        # S·ª≠ d·ª•ng prompt t√πy ch·ªânh ho·∫∑c m·∫∑c ƒë·ªãnh
        prompt = user_prompt if user_prompt else "M√¥ t·∫£ b·ª©c ·∫£nh n√†y"
        
        client = openai.OpenAI(
            base_url="https://api.groq.com/openai/v1",
            api_key=GROQ_API_KEY
        )

        response = client.chat.completions.create(
            model=model_img,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": base64_image
                            }
                        }
                    ]
                }
            ]
        )
        
        return response.choices[0].message.content
    except FileNotFoundError as e:
        return f"L·ªói: {str(e)}"
    except Exception as e:
        return f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}"

# CSV Helper Functions
def load_csv_from_source(source: Union[str, io.BytesIO], max_rows: int = 10000) -> pd.DataFrame:
    """
    Load CSV t·ª´ nhi·ªÅu ngu·ªìn: file path, URL, ho·∫∑c file upload
    
    Args:
        source: ƒê∆∞·ªùng d·∫´n file, URL, ho·∫∑c file object
        max_rows: S·ªë d√≤ng t·ªëi ƒëa ƒë·ªÉ ƒë·ªçc
    
    Returns:
        pandas DataFrame
    """
    encodings = ['utf-8', 'utf-8-sig', 'latin1', 'iso-8859-1', 'cp1252']
    
    # N·∫øu l√† URL
    if isinstance(source, str) and (source.startswith('http://') or source.startswith('https://')):
        try:
            response = requests.get(source, timeout=30)
            response.raise_for_status()
            
            # Th·ª≠ decode v·ªõi nhi·ªÅu encoding
            for encoding in encodings:
                try:
                    csv_content = response.content.decode(encoding)
                    df = pd.read_csv(io.StringIO(csv_content), nrows=max_rows)
                    return df
                except (UnicodeDecodeError, pd.errors.ParserError):
                    continue
            
            raise ValueError("Kh√¥ng th·ªÉ ƒë·ªçc CSV t·ª´ URL v·ªõi c√°c encoding ph·ªï bi·∫øn")
        
        except requests.RequestException as e:
            raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i CSV t·ª´ URL: {str(e)}")
    
    # N·∫øu l√† file path
    elif isinstance(source, str):
        if not os.path.exists(source):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file: {source}")
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc file
        file_size = os.path.getsize(source)
        if file_size > 10 * 1024 * 1024:  # 10MB
            raise ValueError(f"File qu√° l·ªõn ({file_size / (1024*1024):.2f}MB). Gi·ªõi h·∫°n 10MB.")
        
        # ƒê·ªçc v·ªõi nhi·ªÅu encoding
        for encoding in encodings:
            try:
                df = pd.read_csv(source, encoding=encoding, nrows=max_rows)
                return df
            except UnicodeDecodeError:
                continue
        
        raise ValueError("Kh√¥ng th·ªÉ ƒë·ªçc file v·ªõi c√°c encoding ph·ªï bi·∫øn")
    
    # N·∫øu l√† file object (uploaded file)
    else:
        for encoding in encodings:
            try:
                source.seek(0)  # Reset file pointer
                df = pd.read_csv(source, encoding=encoding, nrows=max_rows)
                return df
            except UnicodeDecodeError:
                continue
        
        raise ValueError("Kh√¥ng th·ªÉ ƒë·ªçc file v·ªõi c√°c encoding ph·ªï bi·∫øn")


def summarize_dataset(df: pd.DataFrame) -> str:
    """T√≥m t·∫Øt th√¥ng tin dataset"""
    summary = f"""
üìä T·ªîNG QUAN DATASET
{'='*50}
Th√¥ng tin c∆° b·∫£n:
  - S·ªë d√≤ng: {len(df):,}
  - S·ªë c·ªôt: {len(df.columns)}
  - T√™n c√°c c·ªôt: {', '.join(df.columns.tolist())}

Ki·ªÉu d·ªØ li·ªáu:
{df.dtypes.value_counts().to_string()}

B·ªô nh·ªõ s·ª≠ d·ª•ng: {df.memory_usage(deep=True).sum() / 1024:.2f} KB

C√°c c·ªôt s·ªë: {', '.join(df.select_dtypes(include=['int64', 'float64']).columns.tolist())}
C√°c c·ªôt text: {', '.join(df.select_dtypes(include=['object']).columns.tolist())}
"""
    return summary


def basic_stats(df: pd.DataFrame) -> str:
    """Th·ªëng k√™ c∆° b·∫£n cho c√°c c·ªôt s·ªë"""
    numeric_cols = df.select_dtypes(include=['int64', 'float64'])
    
    if len(numeric_cols.columns) == 0:
        return "Dataset kh√¥ng c√≥ c·ªôt s·ªë n√†o ƒë·ªÉ th·ªëng k√™."
    
    stats = f"""
TH·ªêNG K√ä C√ÅC C·ªòT S·ªê
{'='*50}
{numeric_cols.describe().to_string()}

PH√ÇN B·ªê:
"""
    
    for col in numeric_cols.columns:
        stats += f"\n{col}:"
        stats += f"\n  - Min: {numeric_cols[col].min()}"
        stats += f"\n  - Max: {numeric_cols[col].max()}"
        stats += f"\n  - Mean: {numeric_cols[col].mean():.2f}"
        stats += f"\n  - Median: {numeric_cols[col].median():.2f}"
        stats += f"\n  - Std Dev: {numeric_cols[col].std():.2f}"
    
    return stats


def find_missing_values(df: pd.DataFrame) -> str:
    """T√¨m c√°c c·ªôt c√≥ missing values"""
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    
    missing_df = pd.DataFrame({
        'C·ªôt': missing.index,
        'S·ªë missing': missing.values,
        'T·ª∑ l·ªá (%)': missing_pct.values
    })
    
    missing_df = missing_df[missing_df['S·ªë missing'] > 0].sort_values('S·ªë missing', ascending=False)
    
    if len(missing_df) == 0:
        return "Dataset kh√¥ng c√≥ gi√° tr·ªã missing!"
    
    result = f"""
PH√ÇN T√çCH MISSING VALUES
{'='*50}
{missing_df.to_string(index=False)}

C·ªôt c√≥ nhi·ªÅu missing nh·∫•t: {missing_df.iloc[0]['C·ªôt']} ({missing_df.iloc[0]['T·ª∑ l·ªá (%)']:.2f}%)
"""
    return result


def plot_histogram(df: pd.DataFrame, column: str) -> str:
    """T·∫°o histogram d·∫°ng text cho m·ªôt c·ªôt s·ªë"""
    if column not in df.columns:
        return f"Kh√¥ng t√¨m th·∫•y c·ªôt '{column}'"
    
    if not pd.api.types.is_numeric_dtype(df[column]):
        return f"C·ªôt '{column}' kh√¥ng ph·∫£i l√† c·ªôt s·ªë"
    
    # Lo·∫°i b·ªè NaN
    data = df[column].dropna()
    
    if len(data) == 0:
        return f"C·ªôt '{column}' kh√¥ng c√≥ d·ªØ li·ªáu"
    
    # T·∫°o histogram text-based
    hist, bins = pd.cut(data, bins=10, retbins=True, duplicates='drop')
    counts = hist.value_counts().sort_index()
    
    max_count = counts.max()
    bar_width = 50
    
    result = f"""
HISTOGRAM: {column}
{'='*60}
Min: {data.min():.2f} | Max: {data.max():.2f} | Mean: {data.mean():.2f}
{'='*60}
"""
    
    for interval, count in counts.items():
        bar_len = int((count / max_count) * bar_width)
        bar = '‚ñà' * bar_len
        result += f"\n{str(interval):30s} | {bar} {count}"
    
    return result


# Process CSV - Main Function
def csv_process(source: Union[str, io.BytesIO], user_question: str = None, max_rows: int = 10000) -> str:
    try:
        # Load CSV t·ª´ source
        df = load_csv_from_source(source, max_rows)
        
        # Ki·ªÉm tra empty
        if len(df) == 0:
            return "‚ùå File CSV kh√¥ng c√≥ d·ªØ li·ªáu"
        
        # N·∫øu kh√¥ng c√≥ c√¢u h·ªèi, tr·∫£ v·ªÅ t·ªïng quan
        if not user_question:
            return summarize_dataset(df) + "\n\n" + df.head(10).to_string()
        
        # X·ª≠ l√Ω c√¢u h·ªèi theo keyword
        question_lower = user_question.lower()
        
        # Summarize dataset
        if any(word in question_lower for word in ['summarize', 't√≥m t·∫Øt', 'overview', 't·ªïng quan']):
            return summarize_dataset(df) + "\n\nüìã D·ªÆ LI·ªÜU M·∫™U:\n" + df.head().to_string()
        
        # Basic stats
        elif any(word in question_lower for word in ['basic stats', 'th·ªëng k√™', 'statistics', 'stats']):
            return basic_stats(df)
        
        # Missing values
        elif any(word in question_lower for word in ['missing', 'null', 'nan', 'thi·∫øu']):
            return find_missing_values(df)
        
        # Histogram/plot
        elif 'histogram' in question_lower or 'plot' in question_lower or 'bi·ªÉu ƒë·ªì' in question_lower:
            # T√¨m t√™n c·ªôt trong c√¢u h·ªèi
            for col in df.columns:
                if col.lower() in question_lower:
                    return plot_histogram(df, col)
            
            # N·∫øu kh√¥ng t√¨m th·∫•y c·ªôt, h·ªèi user
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            if numeric_cols:
                return f"Vui l√≤ng ch·ªâ ƒë·ªãnh c·ªôt c·∫ßn v·∫Ω histogram. C√°c c·ªôt s·ªë: {', '.join(numeric_cols)}"
            else:
                return "Dataset kh√¥ng c√≥ c·ªôt s·ªë n√†o ƒë·ªÉ v·∫Ω histogram"
        
        # C√¢u h·ªèi t√πy ch·ªânh - d√πng AI
        else:
            return analyze_with_ai(df, user_question)
        
    except (FileNotFoundError, ValueError) as e:
        return f"‚ùå L·ªói: {str(e)}"
    except pd.errors.EmptyDataError:
        return "‚ùå File CSV tr·ªëng"
    except pd.errors.ParserError:
        return "‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file CSV. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng."
    except Exception as e:
        return f"‚ùå L·ªói x·ª≠ l√Ω CSV: {str(e)}"


def analyze_with_ai(df: pd.DataFrame, question: str) -> str:
    """S·ª≠ d·ª•ng AI ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ dataset"""
    try:
        # T·∫°o context v·ªÅ dataset
        context = f"""
Th√¥ng tin dataset:
- S·ªë d√≤ng: {len(df)}
- S·ªë c·ªôt: {len(df.columns)}
- C√°c c·ªôt: {', '.join(df.columns.tolist())}

D·ªØ li·ªáu m·∫´u (5 d√≤ng ƒë·∫ßu):
{df.head().to_string()}

Th·ªëng k√™ c∆° b·∫£n:
{df.describe().to_string()}
"""
        
        client = openai.OpenAI(
            base_url="https://api.groq.com/openai/v1",
            api_key=GROQ_API_KEY
        )
        
        response = client.chat.completions.create(
            model=model_text,
            messages=[
                {"role": "system", "content": "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch d·ªØ li·ªáu. Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c."},
                {"role": "user", "content": f"{context}\n\nC√¢u h·ªèi: {question}"}
            ]
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        return f"‚ùå L·ªói ph√¢n t√≠ch v·ªõi AI: {str(e)}"
